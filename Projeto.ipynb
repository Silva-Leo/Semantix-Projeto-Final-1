{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivos o diretorio input - docker compose up - acesso container namenode e criação de estrutura diretório HDFS\n",
    "e transferencia dos arquivos csv\n",
    "\n",
    "hdfs dfs -mkdir -p /user/spark/projeto_final_basico\n",
    "\n",
    "hdfs dfs -put /input/HIST_PAINEL_COVIDBR_2021_Parte1_06jul2021.csv /user/spark/projeto_final_basico\n",
    "hdfs dfs -put /input/HIST_PAINEL_COVIDBR_2021_Parte2_06jul2021.csv /user/spark/projeto_final_basico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as spark\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession\\\n",
    ".builder\\\n",
    ".appName('Projeto final Básico - Campanha Nacional de Vacinação contra Covid-19')\\\n",
    ".config('spark.some.config.option', 'some-value')\\\n",
    ".enableHiveSupport()\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investiguei um dos arquivos CSV para entender como ele está formatado. Defini para esse projeto dois arquivos csv.\n",
    "> O arquivo parte 1 encobre dados do primeiro semestre de 2021 e o arquivo parte 2 encobre dados dos primeiros 6 dias do mês de julho de 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /user/leo-silva/Documentos/Semantix/spark/projeto_final/HIST_PAINEL_COVIDBR_2021_Parte1_06jul2021.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /user/leo-silva/Documentos/Semantix/spark/projeto_final/HIST_PAINEL_COVIDBR_2021_Parte2_06jul2021.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identificado o formato do arquivo, criei o dataframe através do diretório do hdfs. Visualizei como está Schema e alterei o formato do campo data. (Início dos tratamento dos dados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df = spark.read.csv('hdfs://namenode/user/spark/projeto_final_basico', sep=\";\",header=True, inferSchema=True, ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True)\n",
    "csv_df.show(10, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df_to_unix.show(30, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criei o Banco de dados \"covid\" e particionei por Municipio ( como pede o exercício )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"create database covid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df_to_unix.write.mode('overwrite').partitionBy('municipio').format('csv').saveAsTable('covid.municipio', path='hdfs://namenode:8020/user/hive/warehouse/covid_municipio/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/hive/warehouse/covid_municipio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"USE covid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM municipio\").show(200,vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZAÇÃO 1 | Obitos estados do maior para o menor -> 1º Semestre 2021 (01/01/2021 - 30/06/2021)\n",
    "estado_obitos = spark.sql(\"SELECT estado, MAX(obitosAcumulado) AS obitos FROM municipio WHERE estado IS NOT NULL GROUP BY estado ORDER BY obitos DESC\")\n",
    "estado_obitos.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZAÇÃO 1.1 | Óbitos pelas regiões do Brasil e Brasil como um todo. ( Brasil inicia o ano de 2021 com 195.411 óbitos )\n",
    "regiao_br_obitos = spark.sql(\"select regiao, max(obitosAcumulado) as obitos from municipio group by regiao order by obitos desc\")\n",
    "regiao_br_obitos.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZAÇÃO 2 | Número total de casos novos no fim do primeiro semestre de 2021.\n",
    "casos_novos = spark.sql(\"SELECT estado, sum(casosNovos) AS casos_novos FROM municipio where estado IS NOT NULL group by estado order by casos_novos desc\")\n",
    "casos_novos.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaçaõ 3 | Valor médio de casos novos e óbitos diários no primeiro semestre por estado.\n",
    "casos_obitos_media = spark.sql(\"SELECT estado, ROUND(SUM(casosNovos) / COUNT(data),2) AS media_casos_novos , ROUND(AVG(obitosAcumulado),2) AS media_obitos_diarios FROM municipio WHERE estado IS NOT NULL GROUP BY estado ORDER BY media_casos_novos DESC\")\n",
    "casos_obitos_media.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Após as 3 visualizações criadas, salvei a 1º como tabela HIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estado_obitos.write.format('csv').saveAsTable('Obitos_por_estado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regiao_br_obitos.write.format('csv').saveAsTable('Obitos_por_regiao')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizar as tabelas salvas \n",
    "spark.sql('SHOW TABLES').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A 2ª com formato parquet e compressão snappy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_novos.write.option('compression', 'snappy').parquet('/user/spark/projeto_final_basico/segunda_visualizacao')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conferindo se foi salvo corretamente\n",
    "!hdfs dfs -ls '/user/spark/projeto_final_basico/segunda_visualizacao'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A 3ª em um tópico no Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_obitos_media.selectExpr(\"to_json(struct(*)) AS value\").write.format('kafka').option('kafka.bootstrap.servers', 'kafka:9092').option('topic', 'casos_obitos_media').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = spark.read.format('kafka').option('kafka.bootstrap.servers', 'kafka:9092').option('subscribe','casos_obitos_media').load()\n",
    "\n",
    "topic_media_casos_obitos = topic.select(col('value').cast('string'))\n",
    "topic_media_casos_obitos.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agora criei uma visualização geral no Spark com todos os dados enviados para o HDFS : Síntese de casos, óbitos, incidência e mortalidade\n",
    "\n",
    "##### Não consegui criar com SQL Querys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark_geral = spark.sql('SELECT regiao, estado, MAX(populacaoTCU2019) AS maximo_populacao_2019, MAX(casosAcumulado) AS maximo_casos_acumulados,  MAX(obitosAcumulado) AS maximo_obitos_acumulados FROM municipio GROUP BY  estado, regiao ORDER BY regiao ASC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark_geral.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark_geral2 = spark.sql('SELECT regiao, estado, ROUND(MAX(casosAcumulado)/ MAX(populacaoTCU2019)*100000,0) AS incidencia, ROUND(MAX(obitosAcumulado)/ MAX(populacaoTCU2019)*100000,0) AS mortalidade FROM municipio GROUP BY  estado, regiao ORDER BY regiao ASC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark_geral2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geral = csv_df.groupBy(['regiao', 'estado']).agg({'casosAcumulado':'max', 'obitosAcumulado':'max', 'populacaoTCU2019':'max'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geral.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_renomear_campos = df_geral.withColumnRenamed('max(populacaoTCU2019)','populacao').withColumnRenamed('max(casosAcumulado)', 'casos_acumulados').withColumnRenamed('max(obitosAcumulado)','obitos_acumulados')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geral_completo = (df_renomear_campos.withColumn('incidencia', round(df_renomear_campos['casos_acumulados']/df_renomear_campos['populacao']*100000,1)).withColumn('mortalidade', round(df_renomear_campos['obitos_acumulados']/df_renomear_campos['populacao']*100000,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geral_completo.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salvar a visualização do exercício 6 em um tópico no Elastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = topic_media_casos_obitos\n",
    "df_final.write.format(\"csv\").save('hdfs://namenode/user/spark/projeto_final_basico/visualizacao3/covid_br.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/spark/projeto_final_basico/visualizacao3/covid_br.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -get /user/spark/projeto_final_basico/visualizacao3/covid_br.csv /input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### POR ALGUM MOTIVO QUE DESCONHEÇO E DEPOIS DE MUITA PESQUISA E DIVERSAS TENTATIVAS NÃO COMPREENDI O MOTIVO DO PORQUE O ARQUIVO \"covid_br.csv\" NÃO FOI SALVO EM MINHA MÁQUINA LOCALMENTE..IMPOSSIBILITANDO-ME DE FINALIZAR O PROJETO..\n",
    "\n",
    ">Professor Rodrigo espero que considere todo o projeto até aqui, realmente localmente o arquivo não existe mesmo constando essa mensagem ao dar run na célula \"!hdfs dfs -get /user/spark/projeto_final_basico/visualizacao3/covid_br.csv /input\"\n",
    ">\n",
    ">A mensagem que aparece é : \n",
    ">get: `/input/covid_br.csv/_SUCCESS': File exists`\n",
    ">\n",
    ">get: `/input/covid_br.csv/part-00000-a5470f49-ccb0-44fa-befd-7e89afd283f7-c000.csv': File exists`\n",
    ">\n",
    ">Porém não há nada no diretório..\n",
    ">\n",
    ">Obrigado."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
