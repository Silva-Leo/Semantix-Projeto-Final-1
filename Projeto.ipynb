{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as spark\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession\\\n",
    ".builder\\\n",
    ".appName('Projeto final Básico - Campanha Nacional de Vacinação contra Covid-19')\\\n",
    ".config('spark.some.config.option', 'some-value')\\\n",
    ".enableHiveSupport()\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investiguei um dos arquivos CSV para entender como ele está formatado. Defini para esse projeto dois arquivos csv.\n",
    "> O arquivo parte 1 encobre dados do primeiro semestre de 2021 e o arquivo parte 2 encobre dados dos primeiros 6 dias do mês de julho de 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /user/leo-silva/Documentos/Semantix/spark/projeto_final/HIST_PAINEL_COVIDBR_2021_Parte1_06jul2021.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /user/leo-silva/Documentos/Semantix/spark/projeto_final/HIST_PAINEL_COVIDBR_2021_Parte2_06jul2021.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identificado o formato do arquivo, criei o dataframe através do diretório do hdfs. Visualizei como está Schema. (Início dos tratamento dos dados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------\n",
      " regiao                 | Brasil              \n",
      " estado                 | null                \n",
      " municipio              | null                \n",
      " coduf                  | 76                  \n",
      " codmun                 | null                \n",
      " codRegiaoSaude         | null                \n",
      " nomeRegiaoSaude        | null                \n",
      " data                   | 2021-01-01 00:00:00 \n",
      " semanaEpi              | 53                  \n",
      " populacaoTCU2019       | 210147125           \n",
      " casosAcumulado         | 7700578             \n",
      " casosNovos             | 24605               \n",
      " obitosAcumulado        | 195411              \n",
      " obitosNovos            | 462                 \n",
      " Recuperadosnovos       | 6756284             \n",
      " emAcompanhamentoNovos  | 748883              \n",
      " interior/metropolitana | null                \n",
      "-RECORD 1-------------------------------------\n",
      " regiao                 | Brasil              \n",
      " estado                 | null                \n",
      " municipio              | null                \n",
      " coduf                  | 76                  \n",
      " codmun                 | null                \n",
      " codRegiaoSaude         | null                \n",
      " nomeRegiaoSaude        | null                \n",
      " data                   | 2021-01-02 00:00:00 \n",
      " semanaEpi              | 53                  \n",
      " populacaoTCU2019       | 210147125           \n",
      " casosAcumulado         | 7716405             \n",
      " casosNovos             | 15827               \n",
      " obitosAcumulado        | 195725              \n",
      " obitosNovos            | 314                 \n",
      " Recuperadosnovos       | 6769420             \n",
      " emAcompanhamentoNovos  | 751260              \n",
      " interior/metropolitana | null                \n",
      "-RECORD 2-------------------------------------\n",
      " regiao                 | Brasil              \n",
      " estado                 | null                \n",
      " municipio              | null                \n",
      " coduf                  | 76                  \n",
      " codmun                 | null                \n",
      " codRegiaoSaude         | null                \n",
      " nomeRegiaoSaude        | null                \n",
      " data                   | 2021-01-03 00:00:00 \n",
      " semanaEpi              | 1                   \n",
      " populacaoTCU2019       | 210147125           \n",
      " casosAcumulado         | 7733746             \n",
      " casosNovos             | 17341               \n",
      " obitosAcumulado        | 196018              \n",
      " obitosNovos            | 293                 \n",
      " Recuperadosnovos       | 6813008             \n",
      " emAcompanhamentoNovos  | 724720              \n",
      " interior/metropolitana | null                \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_df = spark.read.csv('hdfs://namenode/user/spark/projeto_final_basico', sep=\";\",header=True, inferSchema=True, ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True)\n",
    "csv_df.show(3, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- regiao: string (nullable = true)\n",
      " |-- estado: string (nullable = true)\n",
      " |-- municipio: string (nullable = true)\n",
      " |-- coduf: integer (nullable = true)\n",
      " |-- codmun: integer (nullable = true)\n",
      " |-- codRegiaoSaude: integer (nullable = true)\n",
      " |-- nomeRegiaoSaude: string (nullable = true)\n",
      " |-- data: timestamp (nullable = true)\n",
      " |-- semanaEpi: integer (nullable = true)\n",
      " |-- populacaoTCU2019: integer (nullable = true)\n",
      " |-- casosAcumulado: decimal(10,0) (nullable = true)\n",
      " |-- casosNovos: integer (nullable = true)\n",
      " |-- obitosAcumulado: integer (nullable = true)\n",
      " |-- obitosNovos: integer (nullable = true)\n",
      " |-- Recuperadosnovos: integer (nullable = true)\n",
      " |-- emAcompanhamentoNovos: integer (nullable = true)\n",
      " |-- interior/metropolitana: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df_to_unix = csv_df.withColumn('data', from_unixtime(unix_timestamp(csv_df.data), 'yyyy-MM-dd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------\n",
      " regiao                 | Brasil     \n",
      " estado                 | null       \n",
      " municipio              | null       \n",
      " coduf                  | 76         \n",
      " codmun                 | null       \n",
      " codRegiaoSaude         | null       \n",
      " nomeRegiaoSaude        | null       \n",
      " data                   | 2021-01-01 \n",
      " semanaEpi              | 53         \n",
      " populacaoTCU2019       | 210147125  \n",
      " casosAcumulado         | 7700578    \n",
      " casosNovos             | 24605      \n",
      " obitosAcumulado        | 195411     \n",
      " obitosNovos            | 462        \n",
      " Recuperadosnovos       | 6756284    \n",
      " emAcompanhamentoNovos  | 748883     \n",
      " interior/metropolitana | null       \n",
      "-RECORD 1----------------------------\n",
      " regiao                 | Brasil     \n",
      " estado                 | null       \n",
      " municipio              | null       \n",
      " coduf                  | 76         \n",
      " codmun                 | null       \n",
      " codRegiaoSaude         | null       \n",
      " nomeRegiaoSaude        | null       \n",
      " data                   | 2021-01-02 \n",
      " semanaEpi              | 53         \n",
      " populacaoTCU2019       | 210147125  \n",
      " casosAcumulado         | 7716405    \n",
      " casosNovos             | 15827      \n",
      " obitosAcumulado        | 195725     \n",
      " obitosNovos            | 314        \n",
      " Recuperadosnovos       | 6769420    \n",
      " emAcompanhamentoNovos  | 751260     \n",
      " interior/metropolitana | null       \n",
      "-RECORD 2----------------------------\n",
      " regiao                 | Brasil     \n",
      " estado                 | null       \n",
      " municipio              | null       \n",
      " coduf                  | 76         \n",
      " codmun                 | null       \n",
      " codRegiaoSaude         | null       \n",
      " nomeRegiaoSaude        | null       \n",
      " data                   | 2021-01-03 \n",
      " semanaEpi              | 1          \n",
      " populacaoTCU2019       | 210147125  \n",
      " casosAcumulado         | 7733746    \n",
      " casosNovos             | 17341      \n",
      " obitosAcumulado        | 196018     \n",
      " obitosNovos            | 293        \n",
      " Recuperadosnovos       | 6813008    \n",
      " emAcompanhamentoNovos  | 724720     \n",
      " interior/metropolitana | null       \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_df_to_unix.show(3, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criei o Banco de dados \"covid\" e particionei por Municipio ( como pede o exercício )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"create database covid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df_to_unix.write.mode('overwrite').partitionBy('municipio').format('csv').saveAsTable('covid.municipio', path='hdfs://namenode:8020/user/hive/warehouse/covid_municipio/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/hive/warehouse/covid_municipio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|       covid|\n",
      "|     default|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE covid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+-----------+\n",
      "|database|        tableName|isTemporary|\n",
      "+--------+-----------------+-----------+\n",
      "|   covid|        municipio|      false|\n",
      "|   covid|obitos_por_estado|      false|\n",
      "|   covid|obitos_por_regiao|      false|\n",
      "+--------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM municipio\").show(200,vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|estado|obitos|\n",
      "+------+------+\n",
      "|    SP|130389|\n",
      "|    RJ| 56192|\n",
      "|    MG| 47148|\n",
      "|    RS| 31867|\n",
      "|    PR| 31692|\n",
      "|    BA| 24428|\n",
      "|    CE| 22791|\n",
      "|    GO| 19485|\n",
      "|    PE| 17953|\n",
      "|    SC| 17146|\n",
      "|    PA| 15624|\n",
      "|    AM| 13349|\n",
      "|    MT| 12000|\n",
      "|    ES| 11582|\n",
      "|    DF|  9322|\n",
      "|    MA|  9190|\n",
      "|    PB|  8724|\n",
      "|    MS|  8400|\n",
      "|    RN|  6853|\n",
      "|    PI|  6662|\n",
      "+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# VISUALIZAÇÃO 1 | Obitos estados do maior para o menor -> 1º Semestre 2021 (01/01/2021 - 30/06/2021)\n",
    "estado_obitos = spark.sql(\"SELECT estado, MAX(obitosAcumulado) AS obitos FROM municipio WHERE estado IS NOT NULL GROUP BY estado ORDER BY obitos DESC\")\n",
    "estado_obitos.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|      regiao|obitos|\n",
      "+------------+------+\n",
      "|      Brasil|526892|\n",
      "|     Sudeste|130389|\n",
      "|         Sul| 31867|\n",
      "|    Nordeste| 24428|\n",
      "|Centro-Oeste| 19485|\n",
      "|       Norte| 15624|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# VISUALIZAÇÃO 1.1 | Óbitos pelas regiões do Brasil e Brasil como um todo. ( Brasil inicia o ano de 2021 com 195.411 óbitos )\n",
    "regiao_br_obitos = spark.sql(\"select regiao, max(obitosAcumulado) as obitos from municipio group by regiao order by obitos desc\")\n",
    "regiao_br_obitos.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|estado|casos_novos|\n",
      "+------+-----------+\n",
      "|    SP|    4693850|\n",
      "|    MG|    2586578|\n",
      "|    PR|    1784154|\n",
      "|    RS|    1572480|\n",
      "|    BA|    1296424|\n",
      "|    SC|    1147802|\n",
      "|    CE|    1119174|\n",
      "|    RJ|    1071240|\n",
      "|    GO|     755130|\n",
      "|    PE|     678678|\n",
      "|    MT|     553456|\n",
      "|    ES|     549766|\n",
      "|    PA|     528336|\n",
      "|    PB|     471382|\n",
      "|    RN|     458330|\n",
      "|    MS|     411124|\n",
      "|    AM|     408106|\n",
      "|    DF|     366014|\n",
      "|    PI|     312824|\n",
      "|    RO|     312496|\n",
      "+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# VISUALIZAÇÃO 2 | Número total de casos novos no fim do primeiro semestre de 2021.\n",
    "casos_novos = spark.sql(\"SELECT estado, sum(casosNovos) AS casos_novos FROM municipio where estado IS NOT NULL group by estado order by casos_novos desc\")\n",
    "casos_novos.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+--------------------+\n",
      "|estado|media_casos_novos|media_obitos_diarios|\n",
      "+------+-----------------+--------------------+\n",
      "|    DF|           978.65|             6551.88|\n",
      "|    RJ|            60.94|              847.68|\n",
      "|    SP|             38.8|              253.57|\n",
      "|    ES|            36.75|              204.69|\n",
      "|    AM|            34.64|              352.01|\n",
      "|    CE|            32.18|              164.61|\n",
      "|    AP|            31.37|              161.49|\n",
      "|    RO|            30.95|              152.26|\n",
      "|    RR|            28.35|              153.21|\n",
      "|    MS|            27.48|              121.27|\n",
      "|    PR|            23.79|                91.4|\n",
      "|    SE|             21.4|              100.22|\n",
      "|    MT|             20.7|              111.37|\n",
      "|    SC|            20.67|               74.01|\n",
      "|    AC|            20.64|               112.0|\n",
      "|    PA|            19.49|               152.9|\n",
      "|    PE|            19.41|              139.33|\n",
      "|    RS|            16.85|               79.49|\n",
      "|    BA|            16.55|               75.92|\n",
      "|    GO|            16.28|               99.67|\n",
      "+------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualizaçaõ 3 | Valor médio de casos novos e óbitos diários no primeiro semestre por estado.\n",
    "casos_obitos_media = spark.sql(\"SELECT estado, ROUND(SUM(casosNovos) / COUNT(data),2) AS media_casos_novos , ROUND(AVG(obitosAcumulado),2) AS media_obitos_diarios FROM municipio WHERE estado IS NOT NULL GROUP BY estado ORDER BY media_casos_novos DESC\")\n",
    "casos_obitos_media.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Após as 3 visualizações criadas, salvei a 1º como tabela HIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estado_obitos.write.format('csv').saveAsTable('Obitos_por_estado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regiao_br_obitos.write.format('csv').saveAsTable('Obitos_por_regiao')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizar as tabelas salvas \n",
    "spark.sql('SHOW TABLES').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A 2ª com formato parquet e compressão snappy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_novos.write.option('compression', 'snappy').parquet('/user/spark/projeto_final_basico/segunda_visualizacao')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conferindo se foi salvo corretamente\n",
    "!hdfs dfs -ls '/user/spark/projeto_final_basico/segunda_visualizacao'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A 3ª em um tópico no Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_obitos_media.selectExpr(\"to_json(struct(*)) AS value\").write.format('kafka').option('kafka.bootstrap.servers', 'kafka:9092').option('topic', 'casos_obitos_media').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic = spark.read.format('kafka').option('kafka.bootstrap.servers', 'kafka:9092').option('subscribe','casos_obitos_media').load()\n",
    "\n",
    "topic_media_casos_obitos = topic.select(col('value').cast('string'))\n",
    "topic_media_casos_obitos.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agora criei uma visualização geral no Spark com todos os dados enviados para o HDFS : Síntese de casos, óbitos, incidência e mortalidade\n",
    "\n",
    "##### Não consegui criar com SQL Querys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geral = csv_df.groupBy(['regiao', 'estado']).agg({'casosAcumulado':'max', 'obitosAcumulado':'max', 'populacaoTCU2019':'max'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---------------------+-------------------+--------------------+\n",
      "|  regiao|estado|max(populacaoTCU2019)|max(casosAcumulado)|max(obitosAcumulado)|\n",
      "+--------+------+---------------------+-------------------+--------------------+\n",
      "|   Norte|    TO|              1572866|             200243|                3266|\n",
      "|   Norte|    AC|               881935|              85997|                1760|\n",
      "|   Norte|    PA|              8602865|             557708|               15624|\n",
      "|Nordeste|    MA|              7075181|             322052|                9190|\n",
      "|     Sul|    RS|             11377239|            1235914|               31867|\n",
      "| Sudeste|    SP|             45919049|            3809222|              130389|\n",
      "|Nordeste|    PI|              3273227|             299084|                6662|\n",
      "|   Norte|    AP|               845731|             118066|                1857|\n",
      "| Sudeste|    MG|             21168791|            1836198|               47148|\n",
      "|     Sul|    PR|             11433957|            1308643|               31692|\n",
      "+--------+------+---------------------+-------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_geral.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_renomear_campos = df_geral.withColumnRenamed('max(populacaoTCU2019)','populacao').withColumnRenamed('max(casosAcumulado)', 'casos_acumulados').withColumnRenamed('max(obitosAcumulado)','obitos_acumulados')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geral_completo = (df_renomear_campos.withColumn('incidencia', round(df_renomear_campos['casos_acumulados']/df_renomear_campos['populacao']*100000,1)).withColumn('mortalidade', round(df_renomear_campos['obitos_acumulados']/df_renomear_campos['populacao']*100000,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---------+----------------+-----------------+----------+-----------+\n",
      "|  regiao|estado|populacao|casos_acumulados|obitos_acumulados|incidencia|mortalidade|\n",
      "+--------+------+---------+----------------+-----------------+----------+-----------+\n",
      "|   Norte|    TO|  1572866|          200243|             3266|   12731.1|      207.6|\n",
      "|   Norte|    AC|   881935|           85997|             1760|    9750.9|      199.6|\n",
      "|   Norte|    PA|  8602865|          557708|            15624|    6482.8|      181.6|\n",
      "|Nordeste|    MA|  7075181|          322052|             9190|    4551.9|      129.9|\n",
      "|     Sul|    RS| 11377239|         1235914|            31867|   10863.0|      280.1|\n",
      "| Sudeste|    SP| 45919049|         3809222|           130389|    8295.5|      284.0|\n",
      "|Nordeste|    PI|  3273227|          299084|             6662|    9137.3|      203.5|\n",
      "|   Norte|    AP|   845731|          118066|             1857|   13960.2|      219.6|\n",
      "| Sudeste|    MG| 21168791|         1836198|            47148|    8674.1|      222.7|\n",
      "|     Sul|    PR| 11433957|         1308643|            31692|   11445.2|      277.2|\n",
      "+--------+------+---------+----------------+-----------------+----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_geral_completo.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salvar a visualização do exercício 6 em um tópico no Elastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = topic_media_casos_obitos\n",
    "df_final.write.format(\"csv\").save('hdfs://namenode/user/spark/projeto_final_basico/visualizacao3/covid_elastic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/spark/projeto_final_basico/visualizacao3/covid_elastic.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -get /user/spark/projeto_final_basico/visualizacao3/covid_elastic.csv /input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
